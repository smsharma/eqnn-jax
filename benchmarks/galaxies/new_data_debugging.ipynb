{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 09:20:20.065995: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-27 09:20:36.361069: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from dataset_large import get_halo_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Make sure tf does not hog all the GPU memory\n",
    "import tensorflow as tf\n",
    "\n",
    "# Ensure TF does not see GPU and grab all GPU memory\n",
    "tf.config.experimental.set_visible_devices([], \"GPU\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 15000\n"
     ]
    }
   ],
   "source": [
    "features = ['x', 'y', 'z']  # ['x', 'y', 'z', 'Jx', 'Jy', 'Jz', 'vx', 'vy', 'vz', 'M200c']\n",
    "params = ['Omega_m', 'sigma_8']  # ['Omega_m', 'Omega_b', 'h', 'n_s', 'sigma_8']\n",
    "batch_size = 64\n",
    "\n",
    "dataset, num_total, mean, std, mean_params, std_params = get_halo_dataset(batch_size=batch_size,  # Batch size\n",
    "                           num_samples=15_000,  # If not None, will only take a subset of the dataset\n",
    "                           split='train',  # 'train', 'val'\n",
    "                           standardize=True,  # If True, will standardize the features\n",
    "                           return_mean_std=True,  # If True, will return (dataset, num_total, mean, std, mean_params, std_params), else (dataset, num_total)\n",
    "                           seed=42,  # Random seed\n",
    "                           features=features,  # Features to include\n",
    "                           params=params,  # Parameters to include\n",
    "                           tfrecords_path = '/pscratch/sd/c/cuesta/quijote_tfrecords',\n",
    "                        )\n",
    "std = np.array(std)\n",
    "# Print number of samples\n",
    "print(f\"Number of samples: {num_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:07<00:00, 33.29it/s]\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(dataset)\n",
    "\n",
    "x_train, params_train = [], []\n",
    "for _ in tqdm(range(num_total // batch_size)):\n",
    "    x, params = next(iterator)\n",
    "    x_train.append(np.array(x))\n",
    "    params_train.append(np.array(params))\n",
    "\n",
    "x_train = np.concatenate(x_train, axis=0)\n",
    "params_train = np.concatenate(params_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax\n",
    "from flax.training.train_state import TrainState\n",
    "from functools import partial\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "from tqdm import trange\n",
    "\n",
    "replicate = flax.jax_utils.replicate\n",
    "unreplicate = flax.jax_utils.unreplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.utils.equivariant_graph_utils import get_equivariant_graph\n",
    "from models.utils.graph_utils import build_graph, compute_distances, nearest_neighbors\n",
    "from models.segnn import SEGNN\n",
    "from models.gnn import GNN\n",
    "from models.utils.graph_utils import get_apply_pbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import e3nn_jax as e3nn\n",
    "from typing import Dict\n",
    "import jax\n",
    "from models.gnn import GNN\n",
    "from models.segnn import SEGNN\n",
    "from models.egnn import EGNN\n",
    "from models.nequip import NequIP\n",
    "\n",
    "use_pbcs = True\n",
    "apply_pbc = get_apply_pbc(std=std / 1000.,) if use_pbcs else None\n",
    "k = 10\n",
    "n_radial = 64\n",
    "position_features = True\n",
    "r_max = 0.6\n",
    "use_3d_distances = False\n",
    "l_max = 1\n",
    "# TODO: sphharm norm parity between SEGNN and NequIP\n",
    "\n",
    "SEGNN_PARAMS = {\n",
    "    \"d_hidden\": 128,\n",
    "    \"l_max_hidden\": l_max,\n",
    "    \"n_layers\": 3,\n",
    "    \"message_passing_steps\": 3,\n",
    "    \"task\": \"graph\",\n",
    "    \"output_irreps\": e3nn.Irreps(\"1x0e\"),\n",
    "    \"hidden_irreps\": None,\n",
    "    \"message_passing_agg\": \"mean\",\n",
    "    \"readout_agg\": \"mean\",\n",
    "    \"n_outputs\": 2,\n",
    "    \"scalar_activation\": \"gelu\",\n",
    "    \"gate_activation\": \"sigmoid\",\n",
    "    \"mlp_readout_widths\": (4, 2, 2),\n",
    "    \"residual\": False,\n",
    "}\n",
    "\n",
    "GNN_PARAMS = {\n",
    "    \"d_hidden\": 128,\n",
    "    \"message_passing_steps\": 3,\n",
    "    \"n_layers\": 3,\n",
    "    \"activation\": \"gelu\",\n",
    "    \"message_passing_agg\": \"mean\",\n",
    "    \"readout_agg\": \"mean\",\n",
    "    \"mlp_readout_widths\": (4, 2, 2),\n",
    "    \"task\": \"graph\",\n",
    "    \"n_outputs\": 2,\n",
    "    \"norm\": \"none\",\n",
    "    \"position_features\": position_features,\n",
    "    \"residual\": False,\n",
    "}\n",
    "\n",
    "\n",
    "class GraphWrapper(nn.Module):\n",
    "    param_dict: Dict\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "\n",
    "        positions = e3nn.IrrepsArray(\"1o\", x.nodes[..., :3])\n",
    "        \n",
    "        if x.nodes.shape[-1] == 3:\n",
    "            nodes = e3nn.IrrepsArray(\"1o\", x.nodes[..., :])\n",
    "            velocities = None\n",
    "        else:\n",
    "            nodes = e3nn.IrrepsArray(\"1o + 1o\", x.nodes[..., :])\n",
    "            velocities = e3nn.IrrepsArray(\"1o\", x.nodes[..., 3:6])\n",
    "\n",
    "        \n",
    "        st_graph = get_equivariant_graph(\n",
    "            node_features=nodes,\n",
    "            positions=positions,\n",
    "            velocities=None,\n",
    "            steerable_velocities=False,\n",
    "            senders=x.senders,\n",
    "            receivers=x.receivers,\n",
    "            n_node=x.n_node,\n",
    "            n_edge=x.n_edge,\n",
    "            globals=x.globals,\n",
    "            edges=None,\n",
    "            lmax_attributes=l_max,\n",
    "            apply_pbc=apply_pbc,\n",
    "            n_radial_basis=n_radial,\n",
    "            r_max=r_max,\n",
    "        )\n",
    "        \n",
    "        return jax.vmap(SEGNN(**self.param_dict))(st_graph)\n",
    "    \n",
    "class GraphWrapperGNN(nn.Module):\n",
    "    param_dict: Dict\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        return jax.vmap(GNN(**self.param_dict))(x)\n",
    "    \n",
    "class GraphWrapperEGNN(nn.Module):\n",
    "    param_dict: Dict\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        return jax.vmap(EGNN(positions_only=True, n_outputs=2, n_layers=4, apply_pbc=apply_pbc, n_radial_basis=n_radial, r_max=r_max, tanh_out=True))(x)\n",
    "    \n",
    "import jraph\n",
    "import jax.numpy as jnp\n",
    "\n",
    "class GraphWrapperNequIP(nn.Module):\n",
    "    param_dict: Dict\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        if x.nodes.shape[-1] == 3:\n",
    "            ones = jnp.ones(x.nodes[..., :].shape[:2] + (1,))\n",
    "            nodes = jnp.concatenate([x.nodes[..., :], x.nodes[..., :], ones], axis=-1)\n",
    "            nodes = e3nn.IrrepsArray(\"1o + 1o + 1x0e\", nodes)\n",
    "        else:\n",
    "            nodes = e3nn.IrrepsArray(\"1o + 1o + 1x0e\", x.nodes[..., :])\n",
    "        \n",
    "        graph = jraph.GraphsTuple(\n",
    "            n_node=x.n_node,\n",
    "            n_edge=x.n_edge,\n",
    "            edges=None,\n",
    "            globals=x.globals,\n",
    "            nodes=nodes, \n",
    "            senders=x.senders,\n",
    "            receivers=x.receivers)\n",
    "        \n",
    "        return jax.vmap(NequIP(n_outputs=2, n_radial_basis=n_radial, r_cutoff=r_max, sphharm_norm='component'))(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = build_graph(x_train[:2], \n",
    "                None, \n",
    "                k=k, \n",
    "                apply_pbc=apply_pbc,\n",
    "                use_edges=True, \n",
    "                n_radial_basis=n_radial,\n",
    "                r_max=r_max,\n",
    "                use_3d_distances=use_3d_distances,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 700674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_78636/2982461657.py:5: DeprecationWarning: jax.tree_leaves is deprecated: use jax.tree.leaves (jax v0.4.25 or newer) or jax.tree_util.tree_leaves (any JAX version).\n",
      "  print(f\"Number of parameters: {sum([p.size for p in jax.tree_leaves(params)])}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([[ 0.16044052, -0.09306542],\n",
       "       [ 0.15299967, -0.07334966]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GraphWrapperGNN(GNN_PARAMS)\n",
    "\n",
    "out, params = model.init_with_output(jax.random.PRNGKey(0), graph)\n",
    "\n",
    "print(f\"Number of parameters: {sum([p.size for p in jax.tree_leaves(params)])}\")\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 GPUs available\n"
     ]
    }
   ],
   "source": [
    "# Devices\n",
    "num_local_devices = jax.local_device_count()\n",
    "print(f\"{num_local_devices} GPUs available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train state and replicate across devices\n",
    "\n",
    "# Cosine learning rate schedule\n",
    "lr = optax.cosine_decay_schedule(3e-4, 2000)\n",
    "tx = optax.adamw(learning_rate=lr, weight_decay=1e-5)\n",
    "state = TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "pstate = replicate(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_mse(pred_batch, cosmo_batch,):\n",
    "    return np.mean((pred_batch - cosmo_batch) ** 2)\n",
    "\n",
    "@partial(jax.pmap, axis_name=\"batch\",)\n",
    "def train_step(state, halo_batch, cosmo_batch,):\n",
    "\n",
    "    halo_graph = build_graph(halo_batch, \n",
    "                None, \n",
    "                k=k, \n",
    "                use_edges=True, \n",
    "                apply_pbc=apply_pbc,\n",
    "                n_radial_basis=n_radial,\n",
    "                r_max=r_max,\n",
    "                use_3d_distances=use_3d_distances,\n",
    "    )\n",
    "    \n",
    "    def loss_fn(params):\n",
    "        outputs = state.apply_fn(params, halo_graph)\n",
    "        loss = loss_mse(outputs, cosmo_batch)\n",
    "        return loss\n",
    "\n",
    "    # Get loss, grads, and update state\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(state.params)\n",
    "    grads = jax.lax.pmean(grads, \"batch\")\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "    metrics = {\"loss\": jax.lax.pmean(loss, \"batch\")}\n",
    "    \n",
    "    return new_state, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]/tmp/ipykernel_78636/1390803243.py:14: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  halo_batch = jax.tree_map(lambda x: np.split(x, num_local_devices, axis=0), halo_batch)\n",
      "/tmp/ipykernel_78636/1390803243.py:15: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  cosmo_batch = jax.tree_map(lambda x: np.split(x, num_local_devices, axis=0), cosmo_batch)\n",
      "  0%|          | 1/2000 [00:08<4:56:17,  8.89s/it, loss=1.0560976]/tmp/ipykernel_78636/1390803243.py:14: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  halo_batch = jax.tree_map(lambda x: np.split(x, num_local_devices, axis=0), halo_batch)\n",
      "/tmp/ipykernel_78636/1390803243.py:15: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  cosmo_batch = jax.tree_map(lambda x: np.split(x, num_local_devices, axis=0), cosmo_batch)\n",
      "100%|██████████| 2000/2000 [04:04<00:00,  8.19it/s, loss=0.24145743]\n"
     ]
    }
   ],
   "source": [
    "n_steps = 2000\n",
    "n_batch = 32\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "with trange(n_steps) as steps:\n",
    "    for step in steps:\n",
    "        key, subkey = jax.random.split(key)\n",
    "        idx = jax.random.choice(key, x_train.shape[0], shape=(n_batch,))\n",
    "        \n",
    "        halo_batch, cosmo_batch = x_train[idx], params_train[idx]\n",
    "\n",
    "        # Split batches across devices\n",
    "        halo_batch = jax.tree_map(lambda x: np.split(x, num_local_devices, axis=0), halo_batch)\n",
    "        cosmo_batch = jax.tree_map(lambda x: np.split(x, num_local_devices, axis=0), cosmo_batch)\n",
    "        halo_batch, cosmo_batch = np.array(halo_batch), np.array(cosmo_batch)\n",
    "\n",
    "        pstate, metrics = train_step(pstate, halo_batch, cosmo_batch)\n",
    "        \n",
    "        steps.set_postfix(loss=unreplicate(metrics[\"loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "features = ['x', 'y', 'z']  # ['x', 'y', 'z', 'Jx', 'Jy', 'Jz', 'vx', 'vy', 'vz', 'M200c']\n",
    "params = ['Omega_m', 'sigma_8']  # ['Omega_m', 'Omega_b', 'h', 'n_s', 'sigma_8']\n",
    "\n",
    "dataset, num_total = get_halo_dataset(\n",
    "    batch_size=32,  # Batch size\n",
    "                        num_samples=250,  # If not None, will only take a subset of the dataset\n",
    "                        split='val',  # 'train', 'val'\n",
    "                        standardize=True,  # If True, will standardize the features\n",
    "                        return_mean_std=False,  # If True, will return (dataset, num_total, mean, std, mean_params, std_params), else (dataset, num_total)\n",
    "                        seed=42,  # Random seed\n",
    "                        features=features,  # Features to include\n",
    "                        params=params, # Parameters to include\n",
    "                        tfrecords_path = '/pscratch/sd/c/cuesta/quijote_tfrecords',\n",
    "                        )\n",
    "\n",
    "iterator = iter(dataset)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "mse_list = []\n",
    "\n",
    "x_val, params_val = [], []\n",
    "for _ in tqdm(range(num_total // batch_size)):\n",
    "    x, params = next(iterator)\n",
    "\n",
    "    # Convert to numpy\n",
    "    x, params = np.array(x), np.array(params)\n",
    "    \n",
    "    # x_val.append(np.array(x))\n",
    "    # params_val.append(np.array(params))\n",
    "\n",
    "    graph = build_graph(x, \n",
    "                None, \n",
    "                k=k, \n",
    "                use_edges=True, \n",
    "                apply_pbc=apply_pbc,\n",
    "                n_radial_basis=n_radial,\n",
    "                r_max=r_max,\n",
    "                use_3d_distances=use_3d_distances,\n",
    "                )\n",
    "\n",
    "    pred = jax.jit(model.apply)(unreplicate(pstate).params, graph)\n",
    "\n",
    "    ax[0].scatter(params[:, 0], pred[:, 0], s=10, color='firebrick')\n",
    "    ax[1].scatter(params[:, 1], pred[:, 1], s=10, color='firebrick')\n",
    "\n",
    "    mse = np.mean((pred - params) ** 2)\n",
    "    mse_list.append(mse)\n",
    "\n",
    "# Diagonal\n",
    "ax[0].plot([-1.5, 1.5], [-1.5, 1.5], color='black')\n",
    "ax[1].plot([-1.5, 1.5], [-1.5, 1.5], color='black')\n",
    "\n",
    "print(f\"Mean MSE: {np.mean(mse_list)}\")\n",
    "\n",
    "# # # Diagonal line\n",
    "# plt.plot([0, 0.5], [0, 0.5])\n",
    "\n",
    "plt.xlabel(\"True\")\n",
    "plt.ylabel(\"Predicted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "equivariant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
